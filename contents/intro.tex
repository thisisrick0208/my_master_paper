% !TEX root = ../main.tex

\chapter{课题背景和研究意义}

本章节将介绍CIS芯片的前端设计和深度学习网络软硬件发展的背景，讨论两者之间的联系以及
研究的内容和意义。

\section{国内外研究现状}
本章节将从描述CIS芯片开始，结合人工智能软硬间发展的历史，引出本课题的背景研究意义。


\subsection{ASIC与CIS芯片}
%TODO 描述ASIC的基本概念
%TODO 描述CIS芯片的基本概念
CMOS Image Sensor（CIS）就是一种基于CMOS工艺制造的图像传感器。
我们现今常见的照相机、安防摄像头、手机摄像头内使用的图像传感器大多都是采用的CIS芯片。
CIS芯片也属于一种专用集成电路（ASIC）。
专用集成电路（ASIC）的全称是Application Specific Integrated Circuit，它是一种用于特定场景或者用途的芯片。
ASIC设计对任何公司来说都是一项高难度的挑战。  

近年，CIS芯片市场供不应求，技术飞速发展。
CMOS图像传感器的总体结构一般由像素阵列、行选通逻辑、列选通逻辑、定时和控制电路、在片模拟信号处理器（ASP）构成，高级的CMOS图像传感器还集成有在片模数转换器（ADC）\cite{2009CMOS}。
其工作过程一般可分为复位、光电转换、积分、读出几部分。  
索尼在2012年推出了全球首款用于消费电子产品的堆叠芯片CIS相机系统（CIS+ISP）。
在2017年的ISSCC大会上，索尼又发布了他们的一款三层堆栈式CIS\parencite{sony2017}。 %引用 
其结构为：顶层的图像感知层、中间的DRAM层和底层的逻辑电路层（CIS+DRAM+ISP）。
堆栈式CIS芯片成为了发展趋势。
随着各厂商的设计能力与工艺的提升，越来越多的IP被集成到CIS芯片的堆栈中，使得现今的CIS芯片功能更加强大和复杂。
由于堆栈式设计的存在，使得我们将深度学习神经网络的运算模块叠加在CIS芯片中成为可能。  

\subsection{深度学习和残差网络}
想要实现深度学习神经网络算法的硬件加速，就先要对深度学习神经网络算法有所了解。
% 简述DL的发展，例举出最经典的一些算法。
卷积神经网络(Convolutional Neural Networks, CNNs)\parencite{cnn1998}和深度神经网络(Deep Neural Networks, DNNs)\cite{dl2015}是目前最先进的两种机器学习算法之一。
%引用 Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-Based Learning Applied to Document Recognition,” Proceedings of the IEEE, vol. 86, no. 11, pp. 2278– 2324, 1998.
%引用 Deep Learning, by Yann L., Yoshua B. & Geoffrey H. (2015) 
它们都属于多层感知器(MLPs)家族，可能包括四种类型的层:卷积层、池化层、归一化层和分类器层\parencite{NN1998}。%引用
%引用  S. Haykin, Neural Networks: A Comprehensive Foundation, 2nd ed. Upper Saddle River, NJ, USA: Prentice Hall PTR, 1998.
回顾深度学习算法的发展史,始于LeNet识别了10个手写的数字。
它是一种广泛用于文档识别的卷积神经网络，由两个卷积层、两个池层和三个分类器层组成。
卷积层实际上是一种滤波器。它可以用来识别输入的特征图的某些特征。
每个局部滤波器都有一个$K_x$x$K_y$尺寸的核，用来处理卷积窗口。
这个卷积窗口将捕获一个输入特征映射中的$K_x$x$K_y$尺寸的输入神经元。
局部滤波器的2D阵列产生一个输出特征映射。
其中的每一个局部滤波器对应一个输出神经元。
深度学习算法的爆发期在2012年至2017年。
2012年多伦多大学的Geoffrey Hinton的课题组首次参加ImageNet图像识别比赛，就通过其构建的卷积神经网络AlexNet\cite{alexnet2012}一举夺得冠军。
他们的成绩大幅超出了第二名（一种SVM）的分类性能。
这使得AlexNet成为近年的一个里程碑。
%引用 Krizhevsky A, Sutskever I, Hinton G E. Imagenet classification with deep convolutional neural networks[C]//Advances in neural information processing systems. 2012: 1097-1105.


% 简述ResNet
% TODO 简述ResNet18的网络结构
近几年的ImageNet比赛中，网络的深度越来越深。
随着网络的加深，产生了梯度消失或者梯度爆炸的问题，导致训练的准确率下降的现象。
这种现象并不是由过拟合导致的，因为过拟合的现象发生时，训练的准确率会表现得非常高。
针对这个问题何恺明博士在2015年提出的ResNet\parencite{resnet2015}很好地解决了这个问题，并在2015年的ImageNet比赛上获得了第一名。
% 引用
ResNet18由4个主要的残差层组成。
整个网络的开头，由7x7的卷积核做卷积运算和3x3的最大池化。
卷积运算有64个输出的通道。设定的步长是2，填充的边框是3。
最大值池化的步长是2，填充的边框是1。
最后输出的图像尺寸是112x112

之后是四个包含两个BasicBlock来做运算的残差层。
每个BasicBlock中都有两个3x3的卷积运算。
它们的深度不同，分别是64、128、256和512。
ResNet可能使用两种不同的残差单元。一种是BasicBlock，用于ResNet18和ResNet34这种浅层网络。
另一种是Bottleneck，用于50层及以上的ResNet。

% 实现哪一种深度学习算法
本课题中，将要通过设计专用的硬件加速器来进行ResNet18的前向计算。
因此，这一小节将分析ResNet18所涉及到的所有基本运算。


\subsection{深度学习运算的硬件}
现在，市场上有许多可用以深度学习神经网络或卷积神经网络的设备。
常见的，我们会使用GPU来进行网络模型的训练或是图像识别的推理。
这些设备有的在云端，有些在工作站或台式设备上，还有些在移动端甚至在我们身上的可穿戴设备上。
这一章节将介绍深度学习运算硬件的发展史，以及如何评价一种深度学习运算硬件的性能好坏的方法。


\subsubsection{深度学习运算硬件的发展史}

感知时代始于2012年。之后谷歌在几千台普通机器上进行了出色的并行CPU有损更新研究。
在这之后的2014年，VGGNet与GoogLenet相继诞生，专为神经网络设计的硬件在90年代初诞生。
CNAPS（1990），它带有64个处理单元和256kB内存，可以在8/16位条件下达到1.6 GCPS的速度（CPS是指每秒连接次数 / connections per second）或在1位条件下达到12.8GCPS的速度\cite{Heemskerk1995OverviewON}。
%引用   Overview of neural hardware

随着大数据和深度学习的发展，用于相关领域的硬件发展趋势也受到业内人士以及专家们的广泛关注。
在半导体领域，2019 ISSCC大会于2月17—21日在美国旧金山开幕。
Facebook首席AI科学家Yann LeCun在会上发表了主题演讲”深度学习硬件：过去、现在和未来“，详细介绍了深度学习研究的发展将如何影响未来硬件架构\cite{Yann2019DeepLH}。
%引用  ISSCC 2019 Deep Learning Hardware: Past, Present, and Future


深度神经网络在图像的识别、分类、检测和分割等领域起到了非常重要的作用。
在各种DNN的网络结构中，卷积神经网络是最常见的一种。
常见的硬件加速方案中，主要有CPU、GPU、FPGA和ASIC。
如图\ref{fig:silicon_alternatives}所示，这些半导体器件在应用灵活度和能效比两方面的关系成反比。  
\begin{figure}[htbp]
    \centering
    \includegraphics[width=15cm,height=4.5cm]{figures/silicon_alternatives.png}
    \caption{半导体器件能效比与灵活度关系示意图}
    \label{fig:silicon_alternatives}
\end{figure}

CPU和GPU的方案依赖于指令集和编译器的支持。
在实际应用中，GPU无论在训练深度学习神经网络的模型，还是图像识别或分类等深度学习的应用上，都比CPU更具有优势。
GPU相比CPU主要具有以下三点优势：
\begin{enumerate}
    \item 提供了更多可供并行计算的基础计算单元。典型地，这里的并行可以理解为单次执行多条指令的算法。
    \item 防存速度更快，因为GPU的每个核都拥有自己独立的cache；
    \item 一般的，GPU比CPU具有更高的浮点数运算能力。
\end{enumerate}
在常见的CPU和GPU平台上，SIMT和SIMD等技术被广泛地应用。
从字面上解释，SIMT是单指令多线程，而SIMD为单指令多数据流。
在SIMD中，向量中的元素处在相同的地址空间。
而SIMT中的每一个线程的寄存器都是私有的。可以通过共享内存或是其他的线程内同步机制来进行通信。
FPGA是可编程逻辑门电路，这种方案既可以实现算法的硬件加速，又可以不用支付ASIC后端开发和流片的成本。
ASIC作为一种专用的硬件加速方案，在性能和功耗，特别是能耗比上，是其他几种方案都无法相比的。
一般地，这种芯片被称为嵌入式神经网络处理器（NPU）。


\subsubsection{评价硬件的影响因子}
上一节中提到了CPU、GPU、FPGA、ASIC四个类型的半导体器件。
那么如何评价这些硬件在深度学习运算上的表现呢？
这一小节总结了主要的评价硬件在深度学习运算上的影响因子。
它们是精准度、吞吐量、延迟、能效比和功耗。

% TODO 精确度 吞吐量和延迟 能效比和功耗
准确度是这类问题中最常见的一个指标。对于图像分类问题，准确度报告为正确分类图像的百分比，而对于目标检测问题，精度报告为平均准确度。
影响准确性的因素包括任务的难度和数据集。因此，准确度在建模的过程中将被用来验证设计的正确性。
吞吐量用于指示在给定时间段内可以处理的数据量或可以完成的任务的执行次数。高吞吐量通常是应用程序的关键。
延迟度量输入数据到达系统和生成结果之间的时间。低延迟对于实时交互应用程序（如虚拟现实、自主导航和机器人技术）是必要的。
吞吐量和延迟通常被认为是可以直接相互派生的。
一般的，在给定能量单位下可处理的数据量或完成任务的执行次数被称为能效比。


近年，通过FPGA和ASIC实现硬件神经网络加速器的方式在功耗和性能上展现出了巨大的优势。
工程师们还想要进一步降低功耗和提升性能受到存储器访问的限制。
本课题关注的是图像识别的应用。用于该应用的最先进的神经网络算法就是卷积神经网络，它的一个重要特性就是权重在神经元之间共享。
这种特性大幅减少了在神经网络的运算中所使用的内存，允许我们在SRAM中完全映射一个卷积神经网络，并且避免访问DRAM来读取权重值。
而CIS芯片的数据通路具有流水线的特性，使得这种设计具有通用的深度学习加速器不具有的并行运算能力。
另外，加速器本身处在数据通路中，将会进一步减少为了读取输入数据和写入输出数据而进行的DRAM访问。   

与机器学习或深度学习相关的边缘计算领域也在今年得到了长足的发展。
例如：由Pete Warden 与 Daniel Situnayake在2019年提出的TinyML就是在毫瓦功率的微处理器上，实现机器学习算法、工具和技术\cite{tinyml2019}。
%引用  TinyML
这一技术是基于Tensorflow Lite在微控器上的软件框架和硬件环境而来的。
一般来说，它只需要几十千字节的存储空间。  

在CIS的领域，物联网中的感知节点数量持续快速增长，产生海量数据。
到2032年，从感官节点产生的信息相当于1020bit/s，远大于人类感官总吞吐量\cite{2020Near}。
%引用  Near-Sensor and In-Sensor Computing
尽管5G网络或是将来的6G网络会为物联网提供高速传输和大带宽。
因此，近传感器和传感器内计算范式的概念被提了出来。
通过它们在传感器和运算单元之间的位置来定义它们输入哪一种类型。
因为感知功能是在有噪声的模拟域中实现的，使用集成在同一域中的模拟处理器来处理传感器数据是非常理想的，这种类型就是传感器内计算。
这种高效率运算系统避免了数模转换，减少了延迟和功耗。


\subsection{ShiDianNao与谷歌TPU}
近几年，市场上的NPU百花齐放。
几乎所有市场上旗舰手机的SoC都嵌入了NPU。
由于各家厂商嵌入在SoC中的NPU各不相同，因此性能和功耗也是天差地别。
各个厂家也是对这个新ip讳莫如深。
公开可查到的有谷歌TPU\cite{2017In}和寒武纪的Diannao\cite{2014DianNao}系列。
它们的设计思想可以作为后续设计的参考。
当前情况下，寒武纪的Diannao系列显得比谷歌的TPU更开放一些。
寒武纪开放了其指令集Cambricon\cite{2016Cambricon}和相应的工具链或者是云服务。
同时，寒武纪的Diannao系列作品的论文也非常完整，特别是ShiDianNao\cite{2015ShiDianNao}对本课题有非常大的借鉴意义。
谷歌TPU方面，我们只能通过Google Colab服务来使用云端的TPUs。
在最新款的谷歌手机上，名为Google Tensor的SoC芯片内置了Google edge TPU来加强机器学习的运算能力。
但谷歌没有开放相关的SDK，因此可以相信这个TPU只是给设备原生的一些应用使用的。


\subsubsection{ShiDianNao和边缘计算}
%TODO 简单介绍shidiannao和Cambricon的原理
在国内，寒武纪的DianNao系列产品可谓是深度学习处理器的先驱。
DianNao是在2014年寒武纪深度学习处理器的开山之作。
它可以被看作是此系列处理器的硬件设计基础。
后来的的DaDianNao、ShiDianNao、PuDianNao则分别适用于服务器端的高性能计算、设备端边缘计算和更加泛化的机器学习算法。
其中，DianNao、DaDianNao和PuDianNao采用了流水线式乘加树的结构。  %引用   


% ShiDianNao的整体架构描述。
% ShiDianNao的整体架构贴图。
% ShiDianNao的DL加速单元架构描述。
% 上述架构的贴图
ShiDianNao是一个放置在CMOS或CCD传感器旁的CNN加速器\cite{2015ShiDianNao}。
它有特定的数据访问模式来避免DRAM的读写访问。
这是的ShiDianNao具有以前最先进的神经网络加速器的60倍的能效比。
它在65纳米的布局下，面积为4.86 $mm^2$，功耗仅为320 mW，但仍然比高端GPU快30倍。

\begin{figure}[htbp]
    \centering
    \includegraphics[width=15cm,height=8cm]{figures/ShiDianNao_arch.png}
    \caption{ShiDianNao的架构图}
    \label{fig:shidiannao_arch}
\end{figure}
ShiDianNao的架构设计如图\ref{fig:shidiannao_arch}所示\cite{2015ShiDianNao}，在整个SoC上类似于一个挂载在总线上的外设。
加速器有两个功能单元，一个NFU用于调节基本神经元操作（乘法、加法和比较），另一个ALU用于执行激活函数计算。
它的核心模块是NFU（神经功能单元）。
NFU是由$P_x$×$P_y$个处理元素（PE）组成的2D网络。
与DianNao系列的其他产品相比，NFU天然地适合2D特征图的拓扑结构。  
神经元（PE）映射的直观方法是将一块$K_x$×$K_y$ PEs（$K_x$×$K_y$是内核大小）分配给单个输出神经元，同时计算所有突触。
但是ShiDianNao中，优化了计算方法。
它将每个输出神经元映射到单个PE，并在连接到同一输出神经元的输入神经元（即突触）之间共享每个PE。  
NFU不能覆盖CNN中的所有计算原语，因此ShiDianNao需要一个轻量级ALU来补充PEs。
ALU实现了16位定点算术运算符，包括除法（用于平均池化和归一化层）和非线性激活函数，如tanh()和sigmoid()（用于卷积层和池化层）。


\paragraph{存储与运算}
%TODO 关于存储Multiple Bank的设计（状态机）
% 11/24 这一段需要重写
在设计嵌入式的深度学习加速器时，无论是系统级使用的存储或者是模块中的缓冲，都要选择合适的存储介质。
在片内运算时所使用的存储，都要选择SRAM，而需要大量存储的静态数据可以放在DRAM中。
一般的，SRAM的读写速度和功耗，都远小于DRAM,但是SRAM的成本很高。
下面是一些现今的新型存储和先进技术。  


除了存储介质的不同外，还可以根据存储与运算的架构设计分类，来区分不同的存储类型。
以这种方式，主要可以分为以下三类：
\begin{enumerate}
    \item 近存运算
    \item 存内运算
    \item 传感器内运算
\end{enumerate}   


\subparagraph{近存运算}
先进的存储技术可以降低高密度存储器（如DRAM）的访问能量。
例如，嵌入式DRAM（eDRAM）\cite{2001Embedded}在芯片上提供高密度存储器，以避免关闭芯片电容的高能耗成本；eDRAM的密度比SRAM高2.85倍，能效比DRAM高321倍（DDR3）\cite{2017Efficient}。
与DRAM相比，eDRAM还提供了更高的带宽和更低的延迟。在DNN处理中，eDRAM可用于在芯片上存储数十兆字节的权重和激活，以避免芯片外访问，如DaDianNao\cite{2015DaDianNao}所示。
eDRAM的缺点是其密度低于片外DRAM，并且会增加芯片成本。
DRAM也可以使用硅通孔（TSV）\cite{2018The}堆叠在芯片顶部，而不是将DRAM集成到芯片本身。
这项技术通常被称为三维存储器，并已以混合存储立方体（HMC）\cite{2012Hybrid}和高带宽存储器（HBM）\cite{HBMDRAM}的形式商业化。
3-D内存提供了一个数量级的更高带宽，并且相对于现有的2-D DRAM，访问能量减少了5倍，因为TSV的电容低于典型的片外互连\cite{2017Efficient}。

\subparagraph{存内运算}
% 这段重写
存内处理与传统架构之间有很大的差异。
DNN处理可以使用矩阵向量乘法执行，
对于传统架构，输入激活向量和权重矩阵都从各自的存储器中读出，并由乘累加阵列处理。
一次可以读取的权重数量受内存接口的限制（例如，读取逻辑和内存端口的数量）。
权重的有限内存带宽也可以限制可并行执行的乘加运算操作的数量（即每个周期的操作），从而限制总吞吐量。
存内运算的架构是将计算移动到存储权重矩阵的内存中进行的；
这可以通过避免读取权重矩阵的成本来帮助减少权重的数据移动；
只从内存中读取计算结果，如部分和或最终输出激活，而不是读取权重；
此外，存内运算架构中的处理还可以增加内存带宽，因为可以并行访问的权重数量不再受内存接口的限制；
理论上，整个权重矩阵可以并行读取和处理\cite{2017Efficient}。  

%这段减少，另加入SRAM和DRAM的存内运算内容
有一种叫做非易失性电阻存储器的存储介质，非常适合做存内运算。
将乘累加运算直接集成到高级非易失性高密度存储器中，并将其用作可编程电阻元件。我们称这种元件为忆阻器\cite{IEEE1971L}。
这种方法的优点包括：由于计算被嵌入内存中，从而减少了数据移动，因此降低了能耗；由于内存和计算可以以与DRAM类似的密度进行密集封装，因此增加了密度[106]。
非易失性电阻存储器器件有几种常见的候选器件，包括相变存储器（PCM）、电阻RAM（RRAM或ReRAM）、导电桥RAM（CBRAM）和自旋转移转矩磁RAM（STT-MRAM）[107]。
这些设备在耐久性（即可以写入多少次）、保留时间、写入电流、密度（即单元大小）、变化和速度方面有不同的权衡。
如[108]所述，使用非易失性电阻存储器进行处理有几个缺点。
首先，它受到前面描述的模拟处理的精度降低和ADC/DAC开销的影响。
第二，阵列尺寸受到连接电阻器件的导线的限制；具体而言，对于大型阵列（例如1k×1k），导线能量占主导地位，并且沿导线的红外压降会降低读取精度。
第三，对电阻器件进行编程的写入能量可能非常昂贵，在某些情况下需要多个脉冲。
最后，电阻器件还可能受到器件间和周期间变化的影响，在整个电导范围内具有非线性电导。


\subparagraph{传感器内运算}
%这段新增


\subsubsection{谷歌TPU的脉动阵列}
%TODO 简单介绍谷歌TPU的脉动阵列原理。
% TPU的架构设计描述
% TPU的架构设计图
如图，主机通过PCIe总线发送TPU指令到指令缓冲区。其内部模块通过256位宽的数据通路连接。
谷歌TPU的核心部件是图中右侧的矩阵乘法单元。
它包括了256x256个乘加器，可以执行8位无符号和有符号整数乘法和加法。
在计算得到16位无符号或有符号整数后，保存在32位累加器中。
TPU在运算时的核心思想是利用脉动阵列来减少缓冲区和SRAM的读写访问。
它的理念是保持矩阵乘加单元一直工作。
TPU拥有4级流水线。同时设计了一组指令来控制访存和运算。其中每条指令在单独的阶段执行，主要的指令如下：
\begin{enumerate}
    \item Read\_Host\_Memory
    \item Read\_Weights
    \item MatrixMultiply/Convolve
    \item Activation
    \item Write\_Host\_Memory
\end{enumerate}
设计遵循了解耦访问和执行的原则。
因此在发出Read\_Weights指令之后，MatrixMultiply指令就可以开始执行，不需要等待Read\_Weights指令完成。
如果Read\_Weights/Activation没有准备好，矩阵乘法单元就会停止。

\paragraph{脉动阵列}
脉动阵列\cite{1978Systolic}是1978年KUNG H T提出的一种应用在片上多处理器的体系结构。
它是由多个相同的、结构简单的计算单元以网状形式连接而成的，具有并行性、规律性和局部通信的特征。
%REF KUNG H T, Leiserson C E. Systolic arrays(for VLSI)[C]. Sparse Matrix Proceedings 1978, Society for Industrial and Applied Mathematics, 1979.
脉动一词来源于心血管。
在冯诺依曼的计算机体系中，指令和数据是一起存储在存储器中。
在运算的过程中，数据从存储器中被取出，经过处理器的处理后再写回到存储器中。
这个过程就像是人体的血液循环一样，因此得名脉动。  
\begin{figure}[htbp]
    \centering
    \includegraphics[]{figures/systolic_array.png}
    \caption{脉动阵列原理图}
    \label{systolic}
\end{figure}   

当运算的过程是由多个可重复的运算单元组成时，每一个运算单元都会与一个或者多个周围的单元发生数据交互，结果或将存储在运算单元内部，或将传递给下一个单元。
在脉动阵列中，最重要的是设计一种在运算单元之间联系的规则，并且把高层次的算法映射到每一个单元上。
这里的脉动阵列是使用乘累加器作为其处理单元的。
它非常适合用于如矩阵运算，卷积等等数据流具有一定规律的算法。  

\paragraph{脉动阵列上的卷积运算}
上面两个小节已经分别描述了谷歌TPU\cite{2017In}的架构和普通的脉动阵列做乘加运算的过程。
但是在本课题中需要做的是卷积运算，与矩阵的乘加运算是有区别的。
这一小节将描述谷歌TPU是如何基于脉动阵列执行卷积运算的。
谷歌TPU\cite{2017In}在进行卷积运算时，会把卷积运算转化为二维矩阵乘法。
神经网络处理器会把激活输入和权重输入放到矩阵的行和列中。
激活值和权重在脉动阵列中向右和向下传播（从矩阵左边和上边输入），直到到达指定的运算单元的指定寄存器中。
一旦运算单元中的输入数据都放好，通过控制信号，处理器就会将输入数据经过计算得到输出数据。
这里举一个在谷歌TPU中5x5的激活输入使用3x3的卷积核做卷积的例子，这个输入是170x170大小原图的一部分\cite{2017Rotating,2016BATCH,0Computing}。
如图\ref{fig:activation_and_kernel}是输入的激活和卷积核。
\begin{figure}[htbp]
    \centering
    \includegraphics[]{figures/activation_and_kernel.png}
    \caption{输入和卷积核}
    \label{fig:activation_and_kernel}
\end{figure} 

在处理器把激活数据矩阵送进脉动阵列前，会先把矩阵重组为二维的矩阵，然后放入一个向量队列中，然后由多路复用器通过某种算法输入激活。
向量输入队列的排序如图\ref{fig:vector_inputs}所示.
\begin{figure}[htbp]
    \centering
    \includegraphics[]{figures/vector_inputs.png}
    \caption{输入和卷积核}
    \label{fig:vector_inputs}
\end{figure} 

在送入输入数据之前，处理器会执行“旋转卷积核”\cite{2017Rotating}的操作。
具体对3x3的卷积核做“旋转”的操作后，得到下图\ref{fig:rotated_kernel}中的结果。
\begin{figure}[htbp]
    \centering
    \includegraphics[width=12cm,height=1.8cm]{figures/rotated_kernel.png}
    \caption{输入和卷积核}
    \label{fig:rotated_kernel}
\end{figure} 

多路复用器对应卷积核选取输入数据的排序如下图\ref{fig:multiplexor}所示。
\begin{figure}[htbp]
    \centering
    \includegraphics[]{figures/multiplexor.png}
    \caption{输入和卷积核}
    \label{fig:multiplexor}
\end{figure} 

在脉动阵列运行时，模块中的脉动数据配置子模块会根据向量输入队列和多路复用器来依次输入数据到脉动阵列中。
下面图\ref{fig:systolic_data_col0}和图\ref{fig:systolic_data_col4}分别是多路复用器选择的第一组数据和第四组数据。
激活输入矩阵按照卷积核的大小，切割成四组数据，分别按四路放置于向量输入队列中。
多路复用器中的编码就是指的向量输入队列的编号。
可以推论，向量输入会按照脉动的频率向脉动阵列中输入数据，在输入时使用多路复用器来选择使用哪一路的数据。
\begin{figure}[htbp]
    \centering
    \includegraphics[width=12cm]{figures/systolic_data_col0.png}
    \caption{多路复用器选择第0列数据}
    \label{fig:systolic_data_col0}
\end{figure} 

\begin{figure}[htbp]
    \centering
    \includegraphics[width=12cm]{figures/systolic_data_col4.png}
    \caption{多路复用器选择第4列数据}
    \label{fig:systolic_data_col4}
\end{figure} 
当输入数据和权重都放到指定的运算单元后，脉动阵列会执行乘法和累加运算。
以此可见，脉动阵列向下传递的是向量输入和部分和。
最终在输入脉动阵列的数据全都经过计算之后，得到输出的值。
详细的完整例子，可见附录B。



\subsection{建模与IC验证}
在IC设计行业，ASIC项目的拖期情况非常严重，其中有些项目会拖期半年以上。
除了项目容易延期，ASIC项目的失败率也非常高。
小米的澎湃S2处理器曾多次流片失败。
研发一款ASIC芯片的道路充满了艰难险阻。
ASIC芯片的特点与简单指令集的CPU完全相反，它被设计出来的目的就是为了实现复杂功能。
所以ASIC芯片的设计一般都是以具体业务作为需求。
这会导致其设计过程非常繁琐。
如图\ref{fig:asic_flow}是ASIC的开发流程图.IC设计公司会在项目初始时完成市场需求设计（MRD）和产品需求设计（PRD）。
\begin{figure}[htbp]
    \centering
    \includegraphics[width=12cm,height=7cm]{figures/asic_flow.jpeg}
    \caption{ASIC开发流程图}
    \label{fig:asic_flow}
\end{figure}
在需求被明确之后，系统工程师就会开始进行架构设计。
这个过程包含了提出顶层架构设计和定义需求中所有用例涉及到的子模块，并且定义各个子模块之间的连接关系、交互方式和时序要求。
在完成芯片的架构设计之后，需要对芯片进行建模。芯片建模是用一种高级语言对芯片的行为进行描述。
一般会采用C/C++或者SystemC来实现建模。
如果涉及到新的算法模块，会要求算法工程师先做出算法的Matlab或者C模型，作为模块开发者和芯片模型的参考依据。   

% 简述IC验证
芯片在流片之前，需要进行完整和详细的IC验证工作。
工程师需要尽可能地找出项目的缺陷，从而降低流片的风险，节省人力和物力资源。
% 简述什么是建模
芯片建模是用一种高级语言对芯片的行为进行描述，常用的高级语言有C/C++或System C。
% 为什么使用python和C/C++建模
使用C-Model进行建模和仿效，是一种低成本且有效的模块级验证方法。
而使用python可以更快速地对算法建模。
在ASIC项目越来越复杂的今天，使用C/C++或者python进行建模来验证算法是保障项目质量的重要手段。
在IC设计公司中，使用软件建模、软件仿真和FPGA仿真都是常见的仿效或者仿真的方法。
C/C++或python建模可以视为仅需要人力成本。
其编码所需的时间较长，而编译所需的时间和仿真所需的时间都很短。
软件仿真的成本在于仿真软件的授权，其编码的规模一般比编写C/C++或python软件的规模小，但它的缺点是仿真所花费的时间非常长。
FPGA设备非常昂贵单台设备的价值可能是数万至数十万元人民币，其编码和仿真所需的时间不多，但其编译所需的时间非常长。
编译一个可烧录的文件经常需要数小时甚至数十小时。

\paragraph{SystemC}
在建模的过程中将会采用SystemC作为建模的语言。
SystemC既是系统级语言，也是硬件描述语言\cite{2008SystemC}。
%引用《SystemC入门》（第二版）
SystemC 由 C++发展而来，是优秀的系统级描述语言，SystemC 单元库结构如图\ref{fig:systemc_libs}所示，它使软硬件工程师可以在统一的环境下同时描述软硬件结构和接口， 它将通信与功能分开，能提供更高设计效率，将集成电路设计引入事务级，提供 更有效的设计流程，从而帮助解决集成电路产业面临的各种问题，从此可以预见 SystemC 的光明前景\cite{SWZ20131553B}。
% 引用 单中伟. 1553B 总线远程终端的 FPGA 程序设计[J]. 现代电子技术, 2013(09): 28-30.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=8cm]{figures/systemc_libs.png}
    \caption{SystemC单元库}
    \label{fig:systemc_libs}
\end{figure} 
传统的硬件设计使用如 Verilog HDL 或者 VHDL 等硬 件描述语言，而软件设计则使用不同的语言，所以在进行软硬件协同验证时比较 困难,并且软件的设计往往要迟于硬件系统证成为制约设计效率的主要因素为提高 工作效率，人们引进了 SystemC 语言，使得在运用 SystemC 对硬件进行建模时， 可以在一开始就能够同时进行软硬件的设计，这是传统的 SoC 设计所比不了的\cite{2010Real}。
%引用 Zhao Changyu, Yan Changxiang, Yu Ping. Real time scheduling of messages on 1553B bus[J]. Optics and Precision Engineering, 2010, 18(3): 732-40.
在下文的“建模和验证”章节，将使用SystemC作为实现本文设计的建模语言，实现系统级建模和算法的仿效。

\section{主要研究内容和意义}

%TODO CIS增强芯片	神经网络推理 系统级建模
%TODO 研究一种CIS片内的神经网络推理模块的系统级建模
本课题的主要工作内容就是在良好的理解计算机系统，以及对深度学习神经网络算法有一定理解的基础上，运用系统级建模的方法，设计和建模一种能够加速CIS芯片片内进行深度学习神经网络推理运算的方案。  
本课题设计的CIS深度学习神经网络运算加速方案，实现了一种基于脉动阵列的ResNet18的神经网络来进行推理。
其整体结构是基于当前比较先进的堆栈式CIS芯片，在CIS片内集成了传感器核心、深度学习神经网络运算模块以及其他的功能模块。
在设计深度学习神经网络加速器时，本文将讨论和设计脉动阵列的设计，以及神经网络算法在脉动阵列上的应用。    


这种设计实践了“软件定义硬件”的新概念。
其设计和建模过程本身就是一种方法学的探索。
在进行芯片的设计时，系统级建模和算法验证是其中的关键部分。
本文通过构建CIS芯片上的深度学习神经网络加速器的系统级模型，将需要处理的诸多硬件信号作为抽象数据类型进行操作，忽略RTL管脚的通信细节，通过调用相应函数来进行模块之间的通信，提高仿真的速度\cite{2007ESL}。
%引用  Weijers J.W, Glassee M, Schuster T. ESL design and HW/SW co-verification of high-end Software Defined Radio platforms[C]. Hardware/Software Codesign and System Synthesis (CODES+ISSS), 2007 5th IEEE/ACM/IFIP International Conference, Salzburg, 2007: 191-196.
在充分描述系统控制逻辑和算法后，通过SystemC来实现建模的工作。
